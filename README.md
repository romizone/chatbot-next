<div align="center">

# <img src="public/icon.svg" width="40" height="40" alt="icon" /> Open Chatbot

### Zero File Leakage AI Chatbot ‚Äî Your Documents Never Leave Your Server

**An AI chatbot that prevents document data leakage.**
PDF, Word, Excel, and image files are **never sent raw** to AI providers.
Only **sliced text JSON chunks** are transmitted ‚Äî and **nothing is stored** by inference APIs.

[![Next.js](https://img.shields.io/badge/Next.js-16-black?style=for-the-badge&logo=next.js)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-19-61DAFB?style=for-the-badge&logo=react)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![AI SDK](https://img.shields.io/badge/AI_SDK-6-FF6B35?style=for-the-badge)](https://sdk.vercel.ai/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

[The Problem](#-the-problem-document-data-leakage) ¬∑ [The Solution](#-how-open-chatbot-solves-it) ¬∑ [How It Works](#-how-it-works-technically) ¬∑ [Quick Start](#-quick-start) ¬∑ [Features](#-full-features)

</div>

---

## üö® The Problem: Document Data Leakage

Many companies and individuals want to leverage AI to analyze internal documents ‚Äî financial reports, contracts, HR data, medical records, legal documents ‚Äî but face serious risks:

| Risk | Explanation |
|------|------------|
| **Files sent raw to cloud** | When uploading PDF/Word to ChatGPT, Claude, or other AI, the original file is sent to their servers |
| **Binary files stored on AI servers** | `.pdf`, `.docx`, `.xlsx` files are stored temporarily or permanently on AI provider infrastructure |
| **Metadata leakage** | Filenames, author info, revision history, hidden comments are all transmitted |
| **No control** | Once a file is sent, you have no control over data retention and usage |
| **Compliance violation** | Violates GDPR, HIPAA, SOC 2, or internal corporate policies |

> **Real-world example:** You upload a Q4 financial report to ChatGPT. The 5MB PDF is sent in full to OpenAI's servers ‚Äî including metadata, embedded images, hidden text, and revision history. You have no idea how long that file is retained.

---

## ‚úÖ How Open Chatbot Solves It

Open Chatbot uses a **Local Processing + Text-Only Inference** architecture that ensures original files never leave your infrastructure:

```
  YOUR INFRASTRUCTURE (On-Premise / VPS)              CLOUD (AI Provider)
  ==========================================          =======================

  üìÑ PDF  üìù DOCX  üìä XLSX  üñºÔ∏è Image              OpenAI / Claude /
       |                                             DeepSeek API
       v
  +-----------------------+                          Only receives:
  | LOCAL FILE PROCESSOR  |                          ‚úÖ Sliced text (JSON)
  | ‚Ä¢ PDF ‚Üí pdftotext     |                          ‚úÖ User questions
  | ‚Ä¢ DOCX ‚Üí mammoth      |     text JSON            ‚úÖ System prompt
  | ‚Ä¢ XLSX ‚Üí SheetJS      | ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫
  | ‚Ä¢ Image ‚Üí Tesseract   |   (max 30KB/file)       Never receives:
  | ‚Ä¢ OCR scanned docs    |                          ‚ùå Original files (binary)
  +-----------------------+                          ‚ùå Images / scans
       |                                             ‚ùå Document metadata
       v                                             ‚ùå Revision history
  üóëÔ∏è File deleted from                               ‚ùå Hidden content
     memory after                                    ‚ùå Embedded objects
     text extraction
```

### Security Principles

| Principle | Implementation |
|-----------|---------------|
| **Files never sent to AI** | Files are processed locally ‚Üí only extracted text is transmitted |
| **Text is sliced** | Each file is capped at max **30KB of text** before being sent as a JSON chunk |
| **No server storage** | Files are buffered in memory, extracted, then **immediately deleted** from temp |
| **Stateless API inference** | DeepSeek, OpenAI, Claude APIs do not store data from API calls |
| **API keys in browser** | Keys stored in browser localStorage, never on the server |
| **Zero binary transfer** | What's sent to AI: `{"role":"system","content":"text..."}` ‚Äî not files |

---

## üî¨ How It Works Technically

### 1. Upload & Local Processing

When a user uploads a file, **all processing happens on your own server**:

```
POST /api/upload  ‚Üí  FormData { files: [File] }
                            |
                            v
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ  file-processor.ts  ‚îÇ
               ‚îÇ                     ‚îÇ
               ‚îÇ  PDF ‚îÄ‚îÄ‚ñ∫ pdftotext  ‚îÇ  CLI tool (poppler)
               ‚îÇ  PDF ‚îÄ‚îÄ‚ñ∫ OCR       ‚îÇ  tesseract CLI (fallback for scanned docs)
               ‚îÇ  DOCX ‚îÄ‚îÄ‚ñ∫ mammoth   ‚îÇ  npm library
               ‚îÇ  DOC ‚îÄ‚îÄ‚ñ∫ word-ext   ‚îÇ  npm library
               ‚îÇ  XLSX ‚îÄ‚îÄ‚ñ∫ SheetJS   ‚îÇ  npm library
               ‚îÇ  IMG ‚îÄ‚îÄ‚ñ∫ tesseract  ‚îÇ  OCR engine
               ‚îÇ  TXT ‚îÄ‚îÄ‚ñ∫ buffer     ‚îÇ  native Node.js
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         v
               { id, filename, text, size }  ‚Üê JSON response
                                               (text only, not the file)
```

**What happens in memory:**
1. File is buffered as a `Buffer` in RAM
2. Text is extracted by the appropriate library
3. Buffer and temp files are **immediately deleted** after extraction
4. Only a `string` of text is returned to the client

### 2. Sending to AI Provider

When the user sends a message, **only JSON text is sent to the AI API**:

```typescript
// What is ACTUALLY sent to the API (from chat/route.ts)
{
  "model": "deepseek-chat",
  "system": "You are an AI assistant...\n\n=== File: report.pdf ===\nExtracted text here (max 30KB)...\n=== End File ===",
  "messages": [
    { "role": "user", "content": "Analyze this financial report" }
  ],
  "max_tokens": 8192,
  "stream": true
}
```

**Notice:**
- There is no `file`, `attachment`, or `binary` in the payload
- The `system` field contains **plain text** from extraction, not a file
- Each file context is **truncated** to max 30,000 characters (`fc.text.slice(0, 30000)`)
- Responses are streamed as `text-delta` chunks ‚Äî not stored on the server

### 3. Why AI Providers Don't Store Your Data

| Provider | API Policy |
|----------|-----------|
| **OpenAI** | Data from API calls is **not used for training** and not permanently stored (unlike the ChatGPT web interface) |
| **Anthropic** | API calls are **not stored** for model training. Limited retention for abuse monitoring only |
| **DeepSeek** | API follows minimal data retention policy for inference |

> **Note:** This applies to usage via **API** (which Open Chatbot uses), not via web interfaces (ChatGPT/Claude web). Web interfaces have different policies.

---

## üìä Comparison: Open Chatbot vs Direct Upload to AI

| Aspect | Upload to ChatGPT/Claude Web | Open Chatbot |
|--------|------------------------------|-------------|
| Original file sent to cloud | ‚úÖ Yes, full file | ‚ùå No, text only |
| Binary/images transmitted | ‚úÖ Yes | ‚ùå No |
| Document metadata transmitted | ‚úÖ Yes (author, revisions) | ‚ùå No |
| Data used for training | ‚ö†Ô∏è Possibly (depends on settings) | ‚ùå No (API mode) |
| File stored on AI server | ‚ö†Ô∏è Temporarily/permanently | ‚ùå No file at all |
| Data retention control | ‚ùå Minimal | ‚úÖ Full (self-hosted) |
| Compliance friendly | ‚ö†Ô∏è Requires review | ‚úÖ Data stays on-premise |
| Data size transmitted | Full file (MBs) | Text only (max 30KB/file) |

---

## üöÄ Quick Start

### Prerequisites

- **Node.js** 18+
- An API key from at least one provider: [DeepSeek](https://platform.deepseek.com/), [OpenAI](https://platform.openai.com/), or [Anthropic](https://console.anthropic.com/)
- *(Optional)* `poppler-utils` and `tesseract-ocr` for PDF OCR

### Install & Run

```bash
# Clone the repository
git clone https://github.com/romizone/chatbot-next.git
cd chatbot-next

# Install dependencies
npm install

# Start development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) ‚Üí click **Settings** ‚Üí select a provider ‚Üí enter your API key ‚Üí start chatting!

### Environment Variables (Optional)

Create a `.env.local` file for server-side fallback keys:

```env
DEEPSEEK_API_KEY=sk-...
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
```

### Install OCR Tools (Optional, for scanned PDFs & images)

```bash
# macOS
brew install poppler tesseract tesseract-lang

# Ubuntu/Debian
sudo apt install poppler-utils tesseract-ocr tesseract-ocr-eng

# Windows (via chocolatey)
choco install poppler tesseract
```

---

## üéØ Full Features

### Multi-Provider AI Engine

| Provider | Models | Max Output |
|----------|--------|------------|
| **DeepSeek** | DeepSeek Chat, DeepSeek Reasoner | 8K - 16K tokens |
| **OpenAI** | GPT-4o, GPT-4o Mini, GPT-4.1, GPT-4.1 Mini | 16K - 32K tokens |
| **Anthropic** | Claude Sonnet 4.5, Claude Haiku 4.5 | 8K - 16K tokens |

### Local Document Processing

| Format | Engine | Capability |
|--------|--------|-----------|
| PDF | `pdftotext` CLI + Tesseract OCR | Text extraction + OCR for scanned PDFs (max 20 pages) |
| DOCX | `mammoth` | Full text and formatting extraction |
| DOC | `word-extractor` | Legacy Word document support |
| XLSX/XLS | `xlsx` (SheetJS) | Spreadsheet to structured text (CSV per sheet) |
| CSV | `xlsx` (SheetJS) | Direct parsing |
| Images | `tesseract` CLI | OCR: PNG, JPG, BMP, TIFF, WebP |
| Text | Native `Buffer` | TXT, MD, JSON, XML, HTML, source code (20+ formats) |

### Core Features

- **Zero File Leakage** ‚Äî Files processed locally, only text sent to AI via JSON
- **Per-Provider API Keys** ‚Äî Each provider has its own key slot in browser localStorage
- **Real-time Connection Indicator** ‚Äî Green/red status in sidebar
- **Auto-Continue** ‚Äî Detects truncated responses and automatically requests continuation
- **LaTeX Math (KaTeX)** ‚Äî Mathematical formula rendering: `$inline$` and `$$block$$`
- **Syntax Highlighting** ‚Äî Code blocks with language detection (Prism theme)
- **Multi-Session** ‚Äî Chat history with multiple sessions
- **File Context Persistence** ‚Äî File contexts preserved per session
- **Responsive UI** ‚Äî Collapsible sidebar, Tailwind CSS + Radix UI
- **Streaming Response** ‚Äî Real-time responses via Vercel AI SDK `streamText`

---

## üèóÔ∏è Architecture

```
chatbot-next/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/route.ts          # Streaming chat endpoint (multi-provider)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ upload/route.ts        # Local file processing endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-page.tsx          # Main orchestrator
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-area.tsx          # Message display area
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-input.tsx         # Input with file upload
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat-message.tsx       # Message bubble
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown-renderer.tsx  # Markdown + KaTeX + syntax highlight
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings-dialog.tsx    # Provider & API key settings
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sidebar.tsx            # Session list + connection indicator
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ welcome-screen.tsx     # Landing screen
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ui/                        # Radix UI primitives
‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ use-chat-store.ts          # State management (localStorage)
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îú‚îÄ‚îÄ constants.ts               # Model list, system prompt, defaults
‚îÇ       ‚îú‚îÄ‚îÄ file-processor.ts          # Local document processing engine
‚îÇ       ‚îî‚îÄ‚îÄ types.ts                   # TypeScript interfaces
‚îú‚îÄ‚îÄ public/
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ README.md
```

### Data Flow

```
User uploads file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
       ‚îÇ
       ‚ñº
POST /api/upload
       ‚îÇ
       ‚ñº
file-processor.ts ‚îÄ‚îÄ [PDF ‚Üí pdftotext] ‚îÄ‚îÄ [DOCX ‚Üí mammoth]
       ‚îÇ              [XLSX ‚Üí SheetJS]     [IMG ‚Üí tesseract OCR]
       ‚îÇ
       ‚ñº
JSON response: { filename, text, size }    ‚Üê text only, file deleted
       ‚îÇ
       ‚ñº
Browser stores text in memory
       ‚îÇ
       ‚ñº
User sends message
       ‚îÇ
       ‚ñº
POST /api/chat ‚îÄ‚îÄ‚ñ∫ { messages, provider, model, apiKey, fileContexts }
       ‚îÇ
       ‚ñº
Build system prompt + file text (sliced max 30KB/file)
       ‚îÇ
       ‚ñº
streamText() to AI Provider ‚îÄ‚îÄ‚ñ∫ text-delta chunks ‚îÄ‚îÄ‚ñ∫ UI
       ‚îÇ
       ‚ñº
AI only receives JSON text, never file binaries
```

---

## üê≥ Deployment

### Self-Hosted (Recommended for Enterprise)

For maximum data privacy, deploy on your own infrastructure:

```bash
npm run build
npm start
```

### Docker

```dockerfile
FROM node:18-alpine

# Install OCR tools (optional)
RUN apk add --no-cache poppler-utils tesseract-ocr

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

```bash
docker build -t open-chatbot .
docker run -p 3000:3000 open-chatbot
```

### Vercel

```bash
npx vercel
```

> **Note:** On Vercel (serverless), OCR features require the Tesseract binary which may not be available. Use Docker deployment for full OCR support.

---

## üîí Security Summary

| Aspect | Detail |
|--------|--------|
| **File Processing** | 100% local on your server, files deleted after extraction |
| **Data to AI Provider** | Plain text JSON chunks only (max 30KB/file) |
| **API Keys** | Stored in browser localStorage, sent per-request via HTTPS |
| **File Binaries** | Never sent to AI providers |
| **Document Metadata** | Stripped during extraction ‚Äî only text content |
| **Temp Files** | Auto-cleanup after processing (using `finally` blocks) |
| **Data Retention** | Chat history in browser localStorage only |
| **Network Payload** | JSON `{ role, content }` ‚Äî not multipart/form-data files |

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|-------|-----------|
| **Framework** | Next.js 16 (App Router, Turbopack) |
| **UI** | React 19, Tailwind CSS 4, Radix UI, Lucide Icons |
| **AI Integration** | Vercel AI SDK 6 (`streamText`, `createUIMessageStream`) |
| **Document Processing** | pdftotext (poppler), mammoth, word-extractor, SheetJS, Tesseract |
| **Math Rendering** | KaTeX, remark-math, rehype-katex |
| **Code Highlighting** | react-syntax-highlighter (Prism) |
| **Language** | TypeScript 5 (strict mode) |

---

## ü§ù Contributing

Contributions are welcome! Feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'feat: add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built to prevent document data leakage when using AI.**

Your files stay on your server. Only text goes to the cloud.

Made with ‚ù§Ô∏è by [Romi Nur Ismanto](https://github.com/romizone)

</div>
